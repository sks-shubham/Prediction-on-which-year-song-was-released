{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import os\n", "import numpy as np\n", "import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The seed will be fixed to 42 for this assigmnet."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["NUM_FEATS = 90"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Net(object):\n", "\t'''\n", "\t'''"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef __init__(self, num_layers, num_units):\n", "\t\t'''\n", "\t\tInitialize the neural network.\n", "\t\tCreate weights and biases."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tHere, we have provided an example structure for the weights and biases.\n", "\t\tIt is a list of weight and bias matrices, in which, the\n", "\t\tdimensions of weights and biases are (assuming 1 input layer, 2 hidden layers, and 1 output layer):\n", "\t\tweights: [(NUM_FEATS, num_units), (num_units, num_units), (num_units, num_units), (num_units, 1)]\n", "\t\tbiases: [(num_units, 1), (num_units, 1), (num_units, 1), (num_units, 1)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tPlease note that this is just an example.\n", "\t\tYou are free to modify or entirely ignore this initialization as per your need.\n", "\t\tAlso you can add more state-tracking variables that might be useful to compute\n", "\t\tthe gradients efficiently."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tParameters\n", "\t\t----------\n", "\t\t\tnum_layers : Number of HIDDEN layers.\n", "\t\t\tnum_units : Number of units in each Hidden layer.\n", "\t\t'''\n", "\t\tself.num_layers = num_layers\n", "\t\tself.num_units = num_units"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tself.biases = []\n", "\t\tself.weights = []\n", "\t\tfor i in range(num_layers):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tif i==0:\n", "\t\t\t\t# Input layer\n", "\t\t\t\tself.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, self.num_units)))\n", "\t\t\telse:\n", "\t\t\t\t# Hidden layer\n", "\t\t\t\tself.weights.append(np.random.uniform(-1, 1, size=(self.num_units, self.num_units)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tself.biases.append(np.random.uniform(-1, 1, size=(self.num_units, 1)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t# Output layer\n", "\t\tself.biases.append(np.random.uniform(-1, 1, size=(1, 1)))\n", "\t\tself.weights.append(np.random.uniform(-1, 1, size=(self.num_units, 1)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef __call__(self, X):\n", "\t\t'''\n", "\t\tForward propagate the input X through the network,\n", "\t\tand return the output."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tNote that for a classification task, the output layer should\n", "\t\tbe a softmax layer. So perform the computations accordingly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tParameters\n", "\t\t----------\n", "\t\t\tX : Input to the network, numpy array of shape m x d\n", "\t\tReturns\n", "\t\t----------\n", "\t\t\ty : Output of the network, numpy array of shape m x 1\n", "\t\t'''\n", "\t\t\n", "\t\ty=np.dot(self.weights, X)+self.biases\n", "\t\ty=np.max(0, y)\n", "\t\treturn y\n", "\t\t# raise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef backward(self, X, y, lamda):\n", "\t\t'''\n", "\t\tCompute and return gradients loss with respect to weights and biases.\n", "\t\t(dL/dW and dL/db)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tParameters\n", "\t\t----------\n", "\t\t\tX : Input to the network, numpy array of shape m x d\n", "\t\t\ty : Output of the network, numpy array of shape m x 1\n", "\t\t\tlamda : Regularization parameter."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tReturns\n", "\t\t----------\n", "\t\t\tdel_W : derivative of loss w.r.t. all weight values (a list of matrices).\n", "\t\t\tdel_b : derivative of loss w.r.t. all bias values (a list of vectors)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tHint: You need to do a forward pass before performing backward pass.\n", "\t\t'''\n", "\t\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Optimizer(object):\n", "\t'''\n", "\t'''"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef __init__(self, learning_rate):\n", "\t\t'''\n", "\t\tCreate a Gradient Descent based optimizer with given\n", "\t\tlearning rate."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tOther parameters can also be passed to create different types of\n", "\t\toptimizers."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tHint: You can use the class members to track various states of the\n", "\t\toptimizer.\n", "\t\t'''\n", "\t\t"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef step(self, weights, biases, delta_weights, delta_biases):\n", "\t\t'''\n", "\t\tParameters\n", "\t\t----------\n", "\t\t\tweights: Current weights of the network.\n", "\t\t\tbiases: Current biases of the network.\n", "\t\t\tdelta_weights: Gradients of weights with respect to loss.\n", "\t\t\tdelta_biases: Gradients of biases with respect to loss.\n", "\t\t'''\n", "\t\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loss_mse(y, y_hat):\n", "\t'''\n", "\tCompute Mean Squared Error (MSE) loss betwee ground-truth and predicted values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tParameters\n", "\t----------\n", "\t\ty : targets, numpy array of shape m x 1\n", "\t\ty_hat : predictions, numpy array of shape m x 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tReturns\n", "\t----------\n", "\t\tMSE loss between y and y_hat.\n", "\t'''\n", "\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loss_regularization(weights, biases):\n", "\t'''\n", "\tCompute l2 regularization loss."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tParameters\n", "\t----------\n", "\t\tweights and biases of the network."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tReturns\n", "\t----------\n", "\t\tl2 regularization loss \n", "\t'''\n", "\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loss_fn(y, y_hat, weights, biases, lamda):\n", "\t'''\n", "\tCompute loss =  loss_mse(..) + lamda * loss_regularization(..)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tParameters\n", "\t----------\n", "\t\ty : targets, numpy array of shape m x 1\n", "\t\ty_hat : predictions, numpy array of shape m x 1\n", "\t\tweights and biases of the network\n", "\t\tlamda: Regularization parameter"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tReturns\n", "\t----------\n", "\t\tl2 regularization loss \n", "\t'''\n", "\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def rmse(y, y_hat):\n", "\t'''\n", "\tCompute Root Mean Squared Error (RMSE) loss betwee ground-truth and predicted values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tParameters\n", "\t----------\n", "\t\ty : targets, numpy array of shape m x 1\n", "\t\ty_hat : predictions, numpy array of shape m x 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tReturns\n", "\t----------\n", "\t\tRMSE between y and y_hat.\n", "\t'''\n", "\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cross_entropy_loss(y, y_hat):\n", "\t'''\n", "\tCompute cross entropy loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tParameters\n", "\t----------\n", "\t\ty : targets, numpy array of shape m x 1\n", "\t\ty_hat : predictions, numpy array of shape m x 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tReturns\n", "\t----------\n", "\t\tcross entropy loss\n", "\t'''\n", "\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(\n", "\tnet, optimizer, lamda, batch_size, max_epochs,\n", "\ttrain_input, train_target,\n", "\tdev_input, dev_target\n", "):\n", "\t'''\n", "\tIn this function, you will perform following steps:\n", "\t\t1. Run gradient descent algorithm for `max_epochs` epochs.\n", "\t\t2. For each bach of the training data\n", "\t\t\t1.1 Compute gradients\n", "\t\t\t1.2 Update weights and biases using step() of optimizer.\n", "\t\t3. Compute RMSE on dev data after running `max_epochs` epochs."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tHere we have added the code to loop over batches and perform backward pass\n", "\tfor each batch in the loop.\n", "\tFor this code also, you are free to heavily modify it.\n", "\t'''"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tm = train_input.shape[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tfor e in range(max_epochs):\n", "\t\tepoch_loss = 0.\n", "\t\tfor i in range(0, m, batch_size):\n", "\t\t\tbatch_input = train_input[i:i+batch_size]\n", "\t\t\tbatch_target = train_target[i:i+batch_size]\n", "\t\t\tpred = net(batch_input)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# Compute gradients of loss w.r.t. weights and biases\n", "\t\t\tdW, db = net.backward(batch_input, batch_target, lamda)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# Get updated weights based on current weights and gradients\n", "\t\t\tweights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# Update model's weights and biases\n", "\t\t\tnet.weights = weights_updated\n", "\t\t\tnet.biases = biases_updated"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# Compute loss for the batch\n", "\t\t\tbatch_loss = loss_fn(batch_target, pred, net.weights, net.biases, lamda)\n", "\t\t\tepoch_loss += batch_loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t#print(e, i, rmse(batch_target, pred), batch_loss)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t#print(e, epoch_loss)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t# Write any early stopping conditions required (only for Part 2)\n", "\t\t# Hint: You can also compute dev_rmse here and use it in the early\n", "\t\t# \t\tstopping condition."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# After running `max_epochs` (for Part 1) epochs OR early stopping (for Part 2), compute the RMSE on dev data.\n", "\tdev_pred = net(dev_input)\n", "\tdev_rmse = rmse(dev_target, dev_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tprint('RMSE on dev data: {:.5f}'.format(dev_rmse))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_test_data_predictions(net, inputs):\n", "\t'''\n", "\tPerform forward pass on test data and get the final predictions that can\n", "\tbe submitted on Kaggle.\n", "\tWrite the final predictions to the part2.csv file."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tParameters\n", "\t----------\n", "\t\tnet : trained neural network\n", "\t\tinputs : test input, numpy array of shape m x d"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tReturns\n", "\t----------\n", "\t\tpredictions (optional): Predictions obtained from forward pass\n", "\t\t\t\t\t\t\t\ton test data, numpy array of shape m x 1\n", "\t'''\n", "\traise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_data():\n", "\t'''\n", "\tRead the train, dev, and test datasets\n", "\t'''"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ttrain_input = pd.read_csv(\"regression/data/train.csv\").drop(columns=['1'])\n", "\ttrain_target = pd.read_csv(\"regression/data/train.csv\", usecols=(0,))\n", "\tdev_input = pd.read_csv(\"regression/data/dev.csv\").drop(columns=['1'])\n", "\tdev_target = pd.read_csv(\"regression/data/dev.csv\", usecols=(0,))\n", "\ttest_input = pd.read_csv(\"regression/data/test.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t#raise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\treturn train_input, train_target, dev_input, dev_target, test_input"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# Hyper-parameters \n", "\tmax_epochs = 50\n", "\tbatch_size = 256\n", "\tlearning_rate = 0.001\n", "\tnum_layers = 2\n", "\tnum_units = 64\n", "\tlamda = 0.1 # Regularization Parameter"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ttrain_input, train_target, dev_input, dev_target, test_input = read_data()\n", "\tnet = Net(num_layers, num_units)\n", "\toptimizer = Optimizer(learning_rate)\n", "\ttrain(\n", "\t\tnet, optimizer, lamda, batch_size, max_epochs,\n", "\t\ttrain_input, train_target,\n", "\t\tdev_input, dev_target\n", "\t)\n", "\tget_test_data_predictions(net, test_input)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "\tmain()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}